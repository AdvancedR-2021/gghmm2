---
title: "reqspec"
author: "Ajay, Esben and Simon"
date: "21/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objectives with the package

Package that can do HMM for us
- A class that contain parameters for the HMM
- EM
- Forward, Backward
- Decode, State prediction, Forecasting, etc.
- Visualization in the context of HMM
- Generating data from HMM
## Functions

-All the functions should work on the new class. 
- The Visualization of the HMM will need the user to provide the generation functions. 
-The same is true for Generating data from HMM

## HMM

##  Forward, Backward
-Both take a numeric vector $X$ and a HMM class
- Both should output a matrix where each entrance for the forward algorithm will
be the prob. of ending up in a state given what we observed before that, and for the 
backward algorithm it should be the prob. of observing the remaing observation given the current state.
This will allow one to calculate the probability of begin in some state given the all thew observation we have. The most important use of these two algorithms is the EM algorithm, which allow us to fit a HMM to some data. 

## EM 
- The input is numeric vector $X$ , a HMM class and a tolerance level. The Purpose of the EM algorithm is to fit a model to some data. In this case the model will be a hidden markov model. There are two steps to the EM algorithm the expetation step and the Maximization step. In the expetation step we find the value of the expected log likelihood. When we have a function that can calculate this, we then in the maximization step  find the parameters that maximizers the function. By repeating thsi proces multiple times we find parameters that fit the data. 
- The output is the new parameters, which have been fitted to the data. This is done by using the forward and backward algorithm to calculate the expected log likelihood for the the expectation step. Then we used theese value to estimate the parameters. Here th4 choice of model becomes important becoes while the transsesion matrix and the stationary ditribution stay the same the paramerts for the emission change quite a lot for the different distribution it can be. So here we fit the parameters directly for the popular distributions like poisson or normal, but for the rest a direct maximization method is used. 
- If times allow we would like the function to plot the new parameters as a mixed distribution against the data. 

##  local Decode
- The input is a numeric vector $X$ , a HMM class, a numeric value equal to a hidden states index $i$ and a positive numerical value which which is the time we will look at $t$. 
- The output is a numerical value of the prob. of the hidden state being equal to $i$ at time $t$. 

## State prediction
- The input is a numeric vector $X$ , a HMM class, a numeric value equal to a hidden states index $i$ and a positive numerical value which which is the time we will look at $t$. 
- The output is a numerical value which is equal to the prob of HMM having the hidden state $i$ at time $t$ in the future. 


## Forecasting
- The input is a numeric vector $X$ , a HMM class, a numeric value equal to a emission $a$ and a positive numerical value which which is the time we will look at $t$. 
- The output is a numerical value wich is eqaul to the prob of HMM emitting $a$ at time $t$ in the future. 
