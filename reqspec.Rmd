---
title: "reqspec"
author: "Ajay, Esben and Simon"
date: "21/10/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Objectives with the package

Package that can do HMM for us
- A class that contain parameters for the HMM
- EM
- Forward, Backward
- Decode, State prediction, Forecasting, etc.
- Visualization in the context of HMM
- Generating data from HMM
## Functions

-All the functions should work on the new class. 
- The Visualization of the HMM will need the user to provide the generation functions. 
-The same is true for Generating data from HMM

## HMM
A S3 class that contaion the parameters for a hidden markov moddel. This would be the stationary distribution, transmission matrix and the distribution of each hidden state with its parameters. The distribution will come as a vector of strings, where each string is the name of the density function to be used for that hidden state. The parameters for each hidden state are kept in a list of list, where each list contain the named parameters for the relevant distribution. It is important that they are named, as we use the do.call function to call the function with the parameters. The class has its own print function, which will print out the parameter for the HHM. The user can use their own density function, if R does not have  then one they want to use. However then it will be required that the function is defined in the current environment as the function "get", is used to get the function.
##  Forward, Backward
-Both take a numeric vector $X$ and a HMM class
- Both should output a matrix where each entrance for the forward algorithm will
be the prob. of ending up in a state given what we observed before that, and for the 
backward algorithm it should be the prob. of observing the remaing observation given the current state.
This will allow one to calculate the probability of begin in some state given the all thew observation we have. The most important use of these two algorithms is the EM algorithm, which allow us to fit a HMM to some data. 

## EM 
- The input is numeric vector $X$ , a HMM class and a tolerance level. The Purpose of the EM algorithm is to fit a model to some data. In this case the model will be a hidden markov model. There are two steps to the EM algorithm the expetation step and the Maximization step. In the expetation step we find the value of the expected log likelihood. When we have a function that can calculate this, we then in the maximization step  find the parameters that maximizers the function. By repeating thsi proces multiple times we find parameters that fit the data. 
- The output is the new parameters, which have been fitted to the data. This is done by using the forward and backward algorithm to calculate the expected log likelihood for the the expectation step. Then we used theese value to estimate the parameters. Here th4 choice of model becomes important becoes while the transsesion matrix and the stationary ditribution stay the same the paramerts for the emission change quite a lot for the different distribution it can be. So here we fit the parameters directly for the popular distributions like poisson or normal, but for the rest a direct maximization method is used. 
- If times allow we would like the function to plot the new parameters as a mixed distribution against the data. 


## State prediction
- The input is a numeric vector $X$ , a HMM class, a numeric value equal to a hidden states index $i$ and a positive numerical value which which is the time we will look at $t$.  
- The output is a numerical value which is equal to the prob of HMM having the hidden state $i$ at time $t$ in the future. 


##  local Decode
- The input is a numeric vector $X$ and a HMM class. 
- The output is a numerical value between $0$ and $1$. It will be the prob. of getting the output $X$ for the Hidden markov model describe in the HMM class. This is done by using the State prediction describe above and the density function that the HMM class contains. 

## Viterbi
- The input is a numeric vector $X$ and a HMM class. It calculates the sequence of states that has the highest prob. of emitting $X$.
-It will return a list containing the sequence of states and the prob. of this sequence. 
We get this by using the Viterbi algorithm, where we construct the most likely path and while doing so keeps tabs how like it is. 


## Forecasting
- The input is a numeric vector $X$ , a HMM class, a numeric value equal to a emission $a$ and a positive numerical value which which is the time we will look at $t$. Note that it is  assumed that the input $X$ has already taken place, so if $X$ has length $h$ then the actual time is $t+h$. 
- The output is a numerical value. It is the prob of getting emission $a$ at time $t+h$ given $X$. 
