[{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Simon Lolk Lauridsen. Author, maintainer. Esben Skipper. Author.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lolk Lauridsen S, Skipper E (2021). gghmm2: gghmm2. https://advancedr-2021.github.io/gghmm2/, https://github.com/AdvancedR-2021/gghmm2.","code":"@Manual{,   title = {gghmm2: gghmm2},   author = {Simon {Lolk Lauridsen} and Esben Skipper},   year = {2021},   note = {https://advancedr-2021.github.io/gghmm2/, https://github.com/AdvancedR-2021/gghmm2}, }"},{"path":"/index.html","id":"gghmm2-","dir":"","previous_headings":"","what":"gghmm2","title":"gghmm2","text":"package meant estimation parameters Hidden Markov Model done usage EM algorithm numerically optimization. Beside package also contains functions fore forecasting, local decoding Viterbi. package comes data set functions package can tested . Project manager: Simon Documentation manager: Esben Quality manager: Ajay Website: https://advancedr-2021.github.io/gghmm2/","code":""},{"path":"/reference/backward.html","id":null,"dir":"Reference","previous_headings":"","what":"Backward Algorithm — backward","title":"Backward Algorithm — backward","text":"Creates vector conditional probabilities uded functions package.","code":""},{"path":"/reference/backward.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Backward Algorithm — backward","text":"","code":"backward(HM,X)"},{"path":"/reference/backward.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Backward Algorithm — backward","text":"HM  HMM class object X Data log_sum True False","code":""},{"path":"/reference/backward.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Backward Algorithm — backward","text":"beta_matrix matrix backward probabilities","code":""},{"path":"/reference/backward.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Backward Algorithm — backward","text":"Creates backward pass data evaluation form conditional probabilities  observations x_t+1,...,x_t given Markov chain state time t.","code":""},{"path":"/reference/earthquakes.html","id":null,"dir":"Reference","previous_headings":"","what":"earthquakes — earthquakes","title":"earthquakes — earthquakes","text":"dataset annual count major earthquakes year 1900 2006. Major earthquakes refer earthquakes magnitude 7 .","code":""},{"path":"/reference/earthquakes.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"earthquakes — earthquakes","text":"","code":"data(earthquakes)"},{"path":"/reference/earthquakes.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"earthquakes — earthquakes","text":"data frame 107 rows 2 variables: Year year measurement n number major earthquakes measueret","code":""},{"path":"/reference/em.html","id":null,"dir":"Reference","previous_headings":"","what":"Expectation–Maximization (EM) Algorithm — em","title":"Expectation–Maximization (EM) Algorithm — em","text":"EM algorithm finds maximum-likelihood estimates HMM.","code":""},{"path":"/reference/em.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Expectation–Maximization (EM) Algorithm — em","text":"","code":"em(HM,X)"},{"path":"/reference/em.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Expectation–Maximization (EM) Algorithm — em","text":"HM HMM object X Data tol tolerance percentage change likelihoods different iterations maxiter maximum number iteration algorithm run","code":""},{"path":"/reference/em.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Expectation–Maximization (EM) Algorithm — em","text":"HMM Returns updated HMM object","code":""},{"path":"/reference/em.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Expectation–Maximization (EM) Algorithm — em","text":"iterative method find local maximum likelihood maximum poosteriori estimates parametersin  statistical model, model depends hidden variables. EM alternates performing expectation step,  creates function expectation log-likelihood evaluated using current estimate parameters,  maximization step, computes parameters, maximize expected log-likelihood found E step.  iterated. numbers iteration user want , dependent tol maxiter parameters. Maxiter maximum number iterations user allow algorithm run tol minimal percentage change likelihood algorithm need see running another iterations. using two parameters, user can control long algorithm run. Note user using distribution normal poisson model, initial values need picked carefully.  numerical maximization used fine optimal parameter, can problem ill chosen starting parameters.","code":""},{"path":"/reference/em.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Expectation–Maximization (EM) Algorithm — em","text":"","code":"X <- earthquakes$n delta = c(0.5,0.5) trans=matrix(c(0.9,0.1,0.1,0.9),2,2) HM = HMM(initial_dist = delta,transmission = trans,           emis_names = c(\"dpois\",\"dpois\"),parameterlist = list(list(lambda=10),list(lambda=30)) ) em(HM=HM,X=X) #> [1] \"Number of iterations: 35\" #> [1] \"This is a Hidden Markov model with 2 hidden states.\" #> [1] \"It has the initial distirbution of:\" #> [1] 1.000000e+00 3.270886e-93 #> [1] \"It has the transmision matrix:\" #>            [,1]      [,2] #> [1,] 0.92837388 0.1190377 #> [2,] 0.07162612 0.8809623 #> [1] \"State 1  has the emission function dpois with parameters lambda = 15.4203141191078\" #> [1] \"State 2  has the emission function dpois with parameters lambda = 26.0170570907938\""},{"path":"/reference/forecast.html","id":null,"dir":"Reference","previous_headings":"","what":"Forecast — forecast","title":"Forecast — forecast","text":"function give probability observing pred_obs time  pred_time.","code":""},{"path":"/reference/forecast.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forecast — forecast","text":"","code":"forecast(pred_obs,pred_time,HM,X)"},{"path":"/reference/forecast.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forecast — forecast","text":"pred_obs observation like predict pred_time time observation occur HM HMM object X vector containing data, model based .","code":""},{"path":"/reference/forecast.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forecast — forecast","text":"probability observing observation time T+n","code":""},{"path":"/reference/forecast.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forecast — forecast","text":"Note time prediction , time model started, observed data observed. n observation data, looking t future n+t away form first hidden state model.","code":""},{"path":"/reference/forward.html","id":null,"dir":"Reference","previous_headings":"","what":"Forward Algorithm — forward","title":"Forward Algorithm — forward","text":"Creates vector forward probabilities used functions package.","code":""},{"path":"/reference/forward.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Forward Algorithm — forward","text":"","code":"forward(HM,X)"},{"path":"/reference/forward.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Forward Algorithm — forward","text":"HM  HMM class object X Data log_sum True False","code":""},{"path":"/reference/forward.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Forward Algorithm — forward","text":"alpha_matrix matrix forward probabilities","code":""},{"path":"/reference/forward.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Forward Algorithm — forward","text":"recursive computation likelihood plays key role likelihood evaluation thus parameter estimation. also used forecasting, decoding model checking. recursive nature forward algorithm much computationally inexpensive brute-force summation possible state sequences.","code":""},{"path":"/reference/HMM.html","id":null,"dir":"Reference","previous_headings":"","what":"The hidden markov model — HMM","title":"The hidden markov model — HMM","text":"S3 class can used define hidden markov model, can used ther functions package.","code":""},{"path":"/reference/HMM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"The hidden markov model — HMM","text":"","code":"HMM(initial_dist,transmission , emis_names, parameterlist)"},{"path":"/reference/HMM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"The hidden markov model — HMM","text":"initial_dist numerical vector stationary distribution transmission numerical matrix containing transmission probabilities emis_names vector names emission functions parameterlist list list, list containing parameters needed corresponding emission function state_names vector names state parm1 vector parameters parm2 vector parameters","code":""},{"path":"/reference/HMM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"The hidden markov model — HMM","text":"HMM class object","code":""},{"path":"/reference/HMM.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"The hidden markov model — HMM","text":"user define parameters behind model. initial distribution, transmission matrix distribution  hidden state parameters. distribution come vector  strings, string name density function used  hidden state. one string defined, one used distribtuion hidden states. user two ways define parameters density functions use. density need  two different parameters, variables parm1 parm2 can used. can either vector value parameter corresponding distribution value parameter distribution single value work. Alternativly user can also give parameters list lists, internal list contain parameters needed corresponding distribution. class print function,  print parameter HHM. user can use density function,  R one want use. However required  function defined current environment. ensure may  necessary users custom function defined  environment HMM function. important note user specify distribution function, necessary quantile parameter called x.","code":""},{"path":"/reference/HMM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"The hidden markov model — HMM","text":"","code":"X <- earthquakes$n delta = c(0.5,0.5) trans=matrix(c(0.9,0.1,0.1,0.9),2,2) HM = HMM(initial_dist = delta,transmission = trans,   emis_names = \"dpois\",parameterlist = list(list(lambda=10), list(lambda=30)) ) HM #> [1] \"This is a Hidden Markov model with 2 hidden states.\" #> [1] \"It has the initial distirbution of:\" #> [1] 0.5 0.5 #> [1] \"It has the transmision matrix:\" #>      [,1] [,2] #> [1,]  0.9  0.1 #> [2,]  0.1  0.9 #> [1] \"State 1  has the emission function dpois with parameters lambda = 10\" #> [1] \"State 2  has the emission function dpois with parameters lambda = 30\""},{"path":"/reference/local_decoder.html","id":null,"dir":"Reference","previous_headings":"","what":"Local decoder — local_decoder","title":"Local decoder — local_decoder","text":"Local_decoder calculate states probable time frame data covers.","code":""},{"path":"/reference/local_decoder.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Local decoder — local_decoder","text":"","code":"local_decoder(X,HM)"},{"path":"/reference/local_decoder.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Local decoder — local_decoder","text":"X Data HM HMM object","code":""},{"path":"/reference/local_decoder.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Local decoder — local_decoder","text":"vector containing  states probable given model data.","code":""},{"path":"/reference/local_decoder.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Local decoder — local_decoder","text":"function uses state_prob function calculate state likely yield data.  Note   local decoder give state probable separate time, yield  sequence state probable emit data. sequence returned Viterbi function.","code":""},{"path":"/reference/print.HMM.html","id":null,"dir":"Reference","previous_headings":"","what":"print.HMM — print.HMM","title":"print.HMM — print.HMM","text":"print descibtion model defined HMM object.","code":""},{"path":"/reference/print.HMM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"print.HMM — print.HMM","text":"","code":"print(HMM)"},{"path":"/reference/print.HMM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"print.HMM — print.HMM","text":"HMM HMM class object.","code":""},{"path":"/reference/print.HMM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"print.HMM — print.HMM","text":" HMM class object","code":""},{"path":"/reference/state_prob.html","id":null,"dir":"Reference","previous_headings":"","what":"State probability — state_prob","title":"State probability — state_prob","text":"state_prob calculate probability state state_time based HMM data.","code":""},{"path":"/reference/state_prob.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"State probability — state_prob","text":"","code":"state_prob(state,state_time,HM,X)"},{"path":"/reference/state_prob.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"State probability — state_prob","text":"state Natural Numbers 1 number hidden states. state_time Natural Numbers 1 number ob. HM HMM object. X vector observations.","code":""},{"path":"/reference/state_prob.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"State probability — state_prob","text":"numerical value 0 1, probability state  time state_time given model data.","code":""},{"path":"/reference/viterbi.html","id":null,"dir":"Reference","previous_headings":"","what":"Viterbi algorithm — viterbi","title":"Viterbi algorithm — viterbi","text":"Finds dynamic programming likely sequence states","code":""},{"path":"/reference/viterbi.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Viterbi algorithm — viterbi","text":"","code":"viterbi(HM,X)"},{"path":"/reference/viterbi.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Viterbi algorithm — viterbi","text":"HM HMM object X Data","code":""},{"path":"/reference/viterbi.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Viterbi algorithm — viterbi","text":"list() Returns list likely sequence states.","code":""},{"path":"/reference/viterbi.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Viterbi algorithm — viterbi","text":"Global decoding dynamic programming. feasible maximize possible states run O(m^t). Thus Viterbi used alternative approach.","code":""},{"path":"/reference/viterbi.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Viterbi algorithm — viterbi","text":"","code":"#X = read.table(\"http://www.hmms-for-time-series.de/second/data/earthquakes.txt\")[,2] delta = c(0.5,0.5) lambdaL=c(10,30) trans=matrix(c(0.9,0.1,0.1,0.9),2,2) hm = HMM(initial_dist = delta,transmision = trans,emis_names = c(\"dpois\",\"dpois\"),parameterslist = list(list(lambda =10),list(lambda =30))) #> Error in HMM(initial_dist = delta, transmision = trans, emis_names = c(\"dpois\",     \"dpois\"), parameterslist = list(list(lambda = 10), list(lambda = 30))): argument \"transmission\" is missing, with no default viterbi(hm,X) #> Error in viterbi(hm, X): object 'hm' not found"}]
